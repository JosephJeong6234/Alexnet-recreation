{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff4f252a-21ad-440f-ae39-14324e98b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import os\n",
    "import cv2 as cv\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de188804-92b0-48dc-a399-0a0d1b7cbf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs to be more reproducable\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3950178c-c6cf-469e-82a0-04d5df75914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageName</th>\n",
       "      <th>label</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n01737021_10059.JPEG</td>\n",
       "      <td>0</td>\n",
       "      <td>n01737021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n01737021_10083.JPEG</td>\n",
       "      <td>0</td>\n",
       "      <td>n01737021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n01737021_10095.JPEG</td>\n",
       "      <td>0</td>\n",
       "      <td>n01737021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n01737021_10163.JPEG</td>\n",
       "      <td>0</td>\n",
       "      <td>n01737021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n01737021_10235.JPEG</td>\n",
       "      <td>0</td>\n",
       "      <td>n01737021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              imageName  label     folder\n",
       "0  n01737021_10059.JPEG      0  n01737021\n",
       "1  n01737021_10083.JPEG      0  n01737021\n",
       "2  n01737021_10095.JPEG      0  n01737021\n",
       "3  n01737021_10163.JPEG      0  n01737021\n",
       "4  n01737021_10235.JPEG      0  n01737021"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageName</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ILSVRC2012_val_00000043.JPEG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ILSVRC2012_val_00000084.JPEG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ILSVRC2012_val_00000098.JPEG</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ILSVRC2012_val_00000100.JPEG</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ILSVRC2012_val_00000133.JPEG</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      imageName  label\n",
       "0  ILSVRC2012_val_00000043.JPEG      1\n",
       "1  ILSVRC2012_val_00000084.JPEG      0\n",
       "2  ILSVRC2012_val_00000098.JPEG      4\n",
       "3  ILSVRC2012_val_00000100.JPEG      8\n",
       "4  ILSVRC2012_val_00000133.JPEG      6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load some data \n",
    "trainNames = pd.read_csv('./imagenet_train20a1.txt', sep=r\"\\s+\", names=[\"imageName\", \"label\"]) #have space inbetween name and label, tab between rows \n",
    "trainNames[\"folder\"] = trainNames[\"imageName\"].str.extract(r\"(n\\d+)_[0-9]+\") #need to seperate out folder for later usage\n",
    "valNames = pd.read_csv('./imagenet_val20.txt', sep=r\"\\s+\", names=[\"imageName\", \"label\"]) #we can do straight\n",
    "display(trainNames.iloc[0:5, :]) #to check if they exist\n",
    "display(valNames.iloc[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69a54ec-a989-469b-b186-976529e48f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 1000\n",
      "0 19 0 19\n"
     ]
    }
   ],
   "source": [
    "#dataset checking\n",
    "print(len(trainNames), len(valNames))\n",
    "print(min(trainNames['label']), max(trainNames['label']), min(valNames['label']), max(valNames['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e7dbd8-9a93-4eef-84b8-992c403ae971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation loading, was inefficent so changed\n",
    "# valPath = \"./imagenet_val20/imagenet_val20\"\n",
    "# def valLoadImage(name, path):\n",
    "#     imagePath = os.path.join(path, name)\n",
    "#     img = cv.imread(imagePath)\n",
    "#     img = cv.resize(img, (224, 224)) #as we want 224 x 224 for alexnet \n",
    "#     if img is None:\n",
    "#         print(f\"Failed to load image: {imagePath}\")\n",
    "#     return img\n",
    "# valFinal = pd.DataFrame()\n",
    "# valFinal[\"image\"] = val[\"imageName\"].apply(lambda x: valLoadImage(x, valPath))\n",
    "# valFinal[\"label\"] = val[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f63e095b-99a0-43f0-8107-475d7ad33da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loading, was inefficent so changed\n",
    "# trainPath = \"./imagenet_train20a/imagenet_train20a\"\n",
    "# def trainLoadImage(name, folder, path): #note that within the train folder we first have image folders which have the images\n",
    "#     newPath = os.path.join(path, folder)\n",
    "#     return valLoadImage(name, newPath)\n",
    "# trainFinal = pd.DataFrame()\n",
    "# trainFinal[\"image\"] = train[[\"imageName\", \"folder\"]].apply(lambda row: trainLoadImage(row[\"imageName\"], row[\"folder\"], trainPath), axis=1)\n",
    "# trainFinal[\"label\"] = train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd32492a-e066-4b81-aa4e-a41572ef4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to actually put into torch rather than numpy, was inefficent so changed\n",
    "# def convert_image(image):\n",
    "#     return torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "# trainFinal[\"image\"] = trainFinal[\"image\"].apply(convert_image)\n",
    "# valFinal[\"image\"] = valFinal[\"image\"].apply(convert_image)\n",
    "\n",
    "# trainFinal[\"label\"] = pd.Series(torch.from_numpy(trainFinal[\"label\"].to_numpy()))\n",
    "# valFinal[\"label\"] = pd.Series(torch.from_numpy(valFinal[\"label\"].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b9415f-7b91-4dd5-8cb2-f0a228ee6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to load better\n",
    "class ImageNetDataset(Dataset):\n",
    "    def __init__(self, df, base_path, is_train=True):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        self.is_train = is_train\n",
    "        if self.is_train:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.4693, 0.4370, 0.3801], #values from below \n",
    "                                     std=[0.2276, 0.2210, 0.2184])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.4693, 0.4370, 0.3801], #assume should be same values\n",
    "                                     std=[0.2276, 0.2210, 0.2184])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        if self.is_train:\n",
    "            folder = row['folder']\n",
    "            img_path = os.path.join(self.base_path, folder, row['imageName'])\n",
    "        else:\n",
    "            img_path = os.path.join(self.base_path, row['imageName'])\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        label = row['label']\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ee4211-b35f-47a3-9cb7-406ba647423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the actual loading\n",
    "train_path = './imagenet_train20a/imagenet_train20a'\n",
    "val_path = './imagenet_val20/imagenet_val20'\n",
    "\n",
    "trainDS = ImageNetDataset(trainNames, train_path, is_train=True)\n",
    "valDS = ImageNetDataset(valNames, val_path, is_train=False)\n",
    "\n",
    "trainLoad = DataLoader(trainDS, batch_size=32, shuffle=True, num_workers=0)\n",
    "valLoad = DataLoader(valDS, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54371af9-a6b8-4367-b5a2-e61f929997b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:20<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.4693, 0.4370, 0.3801])\n",
      "Std: tensor([0.2276, 0.2210, 0.2184])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_mean_std(dataloader):\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_images_count = 0\n",
    "\n",
    "    for images, _ in tqdm(dataloader):\n",
    "        # Images shape: (batch_size, channels, height, width)\n",
    "        batch_samples = images.size(0)  # batch size (number of images)\n",
    "        images = images.view(batch_samples, images.size(1), -1)  # Flatten H and W\n",
    "        \n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += batch_samples\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "\n",
    "    return mean, std\n",
    "mean, std = compute_mean_std(trainLoad)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d0e8f66-bd1b-4545-8ab9-5061c3c37dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class AlexNet(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=11, padding=2, stride=4) #kernel size of 11, stride of 4, padding is 2 if we want to get 55x55\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, padding=2, stride=1) #k=5, s=1, p=2 if we want 27 to 27\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, padding=1, stride=1) #k=3, s=1, p=1 for all 13 to 13 convs \n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 20)\n",
    "\n",
    "        self.relu = nn.LeakyReLU() #first few kept dying so switched to leaky relu \n",
    "        #self.softmax = nn.Softmax(dim=1) #apparently cross entropy loss already does softmax so better to not have\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2) #as we do a 3x3 pooling with stride of 2 \n",
    "        self.dropout = nn.Dropout(p=0.5) #apparently makes overfitting less lkeley\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        Z1 = self.pool(self.relu(self.conv1(x)))\n",
    "        Z2 = self.pool(self.relu(self.conv2(Z1)))\n",
    "        Z3 = self.relu(self.conv3(Z2))\n",
    "        Z4 = self.relu(self.conv4(Z3))\n",
    "        Z5 = self.pool(self.relu(self.conv5(Z4)))\n",
    "        flatten = torch.flatten(Z5, start_dim=1) #so now is batch size, flatten channels \n",
    "        x1 = self.relu(self.fc1(flatten))\n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.relu(self.fc2(x1))\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.fc3(x2)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "831dea44-d2bd-46bb-a4a3-a658ba529984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#make model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AlexNet()\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a2bd63-8b3a-4165-8c56-82696c09bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other stuff we need before actual\n",
    "def calcAcc(X, Y):\n",
    "    predictions = torch.argmax(X, dim=1)\n",
    "    return (predictions == Y).sum().item()/len(Y)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "num_epochs = 50\n",
    "#endEarly = 20 #when I was infinitely looping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d771ab4-ddeb-4d3a-99b3-845825167a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#too expensive so will do data loading, redundant as change process \n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# X = torch.stack(trainFinal[\"image\"].tolist())  # Make sure images are tensors\n",
    "# Y = torch.tensor(trainFinal[\"label\"].values)\n",
    "\n",
    "# trainDS = TensorDataset(X, Y)\n",
    "# trainLoad = DataLoader(trainDS, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "# val_images = torch.stack(valFinal[\"image\"].tolist())\n",
    "# val_labels = torch.tensor(valFinal[\"label\"].values, dtype=torch.long)\n",
    "\n",
    "# valDS = TensorDataset(val_images, val_labels)\n",
    "# valLoad = DataLoader(valDS, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bae9a2b2-4fc5-45fb-8dfa-7fd7c8a930e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 2.8654, Train Acc: 0.1082,  Val Loss: 2.7121, Val Acc: 0.1650\n",
      "Epoch 2/50, Train Loss: 2.5961, Train Acc: 0.1927,  Val Loss: 2.5715, Val Acc: 0.2110\n",
      "Epoch 3/50, Train Loss: 2.3806, Train Acc: 0.2633,  Val Loss: 2.3892, Val Acc: 0.2720\n",
      "Epoch 4/50, Train Loss: 2.1836, Train Acc: 0.3160,  Val Loss: 2.2017, Val Acc: 0.3140\n",
      "Epoch 5/50, Train Loss: 1.9954, Train Acc: 0.3707,  Val Loss: 2.1550, Val Acc: 0.3390\n",
      "Epoch 6/50, Train Loss: 1.8733, Train Acc: 0.3942,  Val Loss: 2.0705, Val Acc: 0.3370\n",
      "Epoch 7/50, Train Loss: 1.7592, Train Acc: 0.4388,  Val Loss: 1.9098, Val Acc: 0.4040\n",
      "Epoch 8/50, Train Loss: 1.6404, Train Acc: 0.4755,  Val Loss: 2.0433, Val Acc: 0.3940\n",
      "Epoch 9/50, Train Loss: 1.5310, Train Acc: 0.5035,  Val Loss: 1.8608, Val Acc: 0.4230\n",
      "Epoch 10/50, Train Loss: 1.4144, Train Acc: 0.5475,  Val Loss: 1.8642, Val Acc: 0.4250\n",
      "Epoch 11/50, Train Loss: 1.3419, Train Acc: 0.5650,  Val Loss: 1.9491, Val Acc: 0.4370\n",
      "Epoch 12/50, Train Loss: 1.2283, Train Acc: 0.6010,  Val Loss: 1.7954, Val Acc: 0.4580\n",
      "Epoch 13/50, Train Loss: 1.1357, Train Acc: 0.6343,  Val Loss: 1.7821, Val Acc: 0.4750\n",
      "Epoch 14/50, Train Loss: 1.0668, Train Acc: 0.6582,  Val Loss: 1.7787, Val Acc: 0.4960\n",
      "Epoch 15/50, Train Loss: 0.9632, Train Acc: 0.6837,  Val Loss: 1.8746, Val Acc: 0.4550\n",
      "Epoch 16/50, Train Loss: 0.8886, Train Acc: 0.7088,  Val Loss: 1.7878, Val Acc: 0.4920\n",
      "Epoch 17/50, Train Loss: 0.7978, Train Acc: 0.7393,  Val Loss: 1.8857, Val Acc: 0.4950\n",
      "Epoch 18/50, Train Loss: 0.7292, Train Acc: 0.7630,  Val Loss: 1.9171, Val Acc: 0.4910\n",
      "Epoch 19/50, Train Loss: 0.6459, Train Acc: 0.7882,  Val Loss: 1.9717, Val Acc: 0.4880\n",
      "Epoch 20/50, Train Loss: 0.5968, Train Acc: 0.8068,  Val Loss: 2.1464, Val Acc: 0.4810\n",
      "Epoch 21/50, Train Loss: 0.5091, Train Acc: 0.8358,  Val Loss: 2.1707, Val Acc: 0.4910\n",
      "Epoch 22/50, Train Loss: 0.4857, Train Acc: 0.8428,  Val Loss: 2.2175, Val Acc: 0.4970\n",
      "Epoch 23/50, Train Loss: 0.4206, Train Acc: 0.8598,  Val Loss: 2.4960, Val Acc: 0.4890\n",
      "Epoch 24/50, Train Loss: 0.4059, Train Acc: 0.8688,  Val Loss: 2.1713, Val Acc: 0.5260\n",
      "Epoch 25/50, Train Loss: 0.3732, Train Acc: 0.8773,  Val Loss: 2.3282, Val Acc: 0.5000\n",
      "Epoch 26/50, Train Loss: 0.3221, Train Acc: 0.8945,  Val Loss: 2.3363, Val Acc: 0.5110\n",
      "Epoch 27/50, Train Loss: 0.2689, Train Acc: 0.9120,  Val Loss: 2.4603, Val Acc: 0.5220\n",
      "Epoch 28/50, Train Loss: 0.2795, Train Acc: 0.9123,  Val Loss: 2.7560, Val Acc: 0.5290\n",
      "Epoch 29/50, Train Loss: 0.2591, Train Acc: 0.9177,  Val Loss: 2.4505, Val Acc: 0.5130\n",
      "Epoch 30/50, Train Loss: 0.2218, Train Acc: 0.9267,  Val Loss: 2.6159, Val Acc: 0.5210\n",
      "Epoch 31/50, Train Loss: 0.2270, Train Acc: 0.9265,  Val Loss: 2.5040, Val Acc: 0.5120\n",
      "Epoch 32/50, Train Loss: 0.2131, Train Acc: 0.9305,  Val Loss: 2.5933, Val Acc: 0.5510\n",
      "Epoch 33/50, Train Loss: 0.1995, Train Acc: 0.9383,  Val Loss: 2.8291, Val Acc: 0.5090\n",
      "Epoch 34/50, Train Loss: 0.2185, Train Acc: 0.9338,  Val Loss: 2.6161, Val Acc: 0.5110\n",
      "Epoch 35/50, Train Loss: 0.1622, Train Acc: 0.9517,  Val Loss: 3.0781, Val Acc: 0.5230\n",
      "Epoch 36/50, Train Loss: 0.1761, Train Acc: 0.9423,  Val Loss: 2.8193, Val Acc: 0.5110\n",
      "Epoch 37/50, Train Loss: 0.1749, Train Acc: 0.9432,  Val Loss: 2.8835, Val Acc: 0.5090\n",
      "Epoch 38/50, Train Loss: 0.1690, Train Acc: 0.9453,  Val Loss: 2.8769, Val Acc: 0.5160\n",
      "Epoch 39/50, Train Loss: 0.1539, Train Acc: 0.9557,  Val Loss: 2.7896, Val Acc: 0.5220\n",
      "Epoch 40/50, Train Loss: 0.1442, Train Acc: 0.9537,  Val Loss: 3.0353, Val Acc: 0.5340\n",
      "Epoch 41/50, Train Loss: 0.1441, Train Acc: 0.9562,  Val Loss: 2.8060, Val Acc: 0.5270\n",
      "Epoch 42/50, Train Loss: 0.1194, Train Acc: 0.9617,  Val Loss: 3.2146, Val Acc: 0.5000\n",
      "Epoch 43/50, Train Loss: 0.1647, Train Acc: 0.9497,  Val Loss: 2.8209, Val Acc: 0.5080\n",
      "Epoch 44/50, Train Loss: 0.1427, Train Acc: 0.9572,  Val Loss: 2.8187, Val Acc: 0.5120\n",
      "Epoch 45/50, Train Loss: 0.1213, Train Acc: 0.9622,  Val Loss: 2.8170, Val Acc: 0.5230\n",
      "Epoch 46/50, Train Loss: 0.1351, Train Acc: 0.9630,  Val Loss: 3.1900, Val Acc: 0.5240\n",
      "Epoch 47/50, Train Loss: 0.1099, Train Acc: 0.9647,  Val Loss: 3.1209, Val Acc: 0.5230\n",
      "Epoch 48/50, Train Loss: 0.1427, Train Acc: 0.9560,  Val Loss: 3.1487, Val Acc: 0.5020\n",
      "Epoch 49/50, Train Loss: 0.1291, Train Acc: 0.9568,  Val Loss: 2.9733, Val Acc: 0.5180\n",
      "Epoch 50/50, Train Loss: 0.1507, Train Acc: 0.9583,  Val Loss: 2.8823, Val Acc: 0.5080\n"
     ]
    }
   ],
   "source": [
    "#actual\n",
    "#currBestAcc = -1\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    trainLoss, correct, total = 0.0, 0, 0 #just set all here\n",
    "    for inputs, labels in trainLoad:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "        trainLoss += loss.item() * inputs.size(0)\n",
    "        batchAcc = calcAcc(outputs, labels)\n",
    "        correct += batchAcc * inputs.size(0)\n",
    "        total += inputs.size(0)\n",
    "    trainLoss /= len(trainLoad.dataset)\n",
    "    trainAcc = correct / total\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valLoss, correct, total = 0.0, 0, 0\n",
    "        for inputs, labels in valLoad:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            valLoss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            batchAcc = calcAcc(outputs, labels)\n",
    "            correct += batchAcc * inputs.size(0)\n",
    "            total += inputs.size(0)\n",
    "    valLoss /= total\n",
    "    valAcc = correct / total\n",
    "\n",
    "    # if valAcc > currBestAcc:\n",
    "    #     currBestAcc = valAcc\n",
    "    #     count = 0\n",
    "    # else:\n",
    "    #     count += 1\n",
    "    #     if count >= endEarly:\n",
    "    #         break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {trainLoss:.4f}, Train Acc: {trainAcc:.4f},  Val Loss: {valLoss:.4f}, Val Acc: {valAcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7055af6c-a32c-4b16-a33d-84a0c0f7b626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#failed first one as too little left in cuda\n",
    "# for epoch in range(num_epochs):\n",
    "#     #set up\n",
    "#     model.train()\n",
    "#     trainLoss = 0.0\n",
    "#     A0 = torch.stack(trainFinal[\"image\"].apply(lambda x: x.to(device)).tolist())\n",
    "#     Y =  torch.tensor(trainFinal[\"label\"].values, dtype=torch.long).to(device)\n",
    "\n",
    "#     #forward\n",
    "#     output = model(A0)\n",
    "#     loss = criterion(output, Y)\n",
    "\n",
    "#     #backward\n",
    "#     optimizer.zero_grad()  # clear the gradients of all optimized variables\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     train_loss += loss.item()\n",
    "\n",
    "#     #see how good with val\n",
    "#     model.eval()\n",
    "#     output = model(valFinal[\"image\"].to(device))\n",
    "#     valLoss = criterion(output, A0)\n",
    "#     valAcc = calculate_accuracy(output, A0)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {trainLoss:.4f}, \"\n",
    "#           f\"Val Acc: {valAcc:.4f}, Val Loss: {valLoss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf494d88-8a48-4df1-bdb0-0f543991ab43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch1]",
   "language": "python",
   "name": "conda-env-torch1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
